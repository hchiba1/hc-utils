#!/usr/bin/env python3
import argparse
import os
import requests
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urlparse
from pathlib import Path
import ftplib

parser = argparse.ArgumentParser(description="Fetch and cache web content.")
parser.add_argument('url', help="URL to fetch")
parser.add_argument('-n', '--no-cache', action='store_true', help="Do not use cache")
args = parser.parse_args()

cache_dir = os.path.expanduser("~/.get/cache")
cache_time_limit = 21600  # 6 hours


def main():
    try:
        # content = fetch_url(args.url, cache_dir, cache_time_limit, not args.no_cache)
        # print(content)
        host = "ftp.ncbi.nlm.nih.gov"
        directory = "/"
        ftp = ftplib.FTP(host)
        ftp.login("anonymous", "")
        ftp.cwd(directory)
        mlsd(ftp)
        ftp.quit()
    except Exception as e:
        print(f"Error fetching URL: {e}")

def dir(ftp):
    ftp.dir()

def mlsd(ftp):
    files = ftp.mlsd()
    for file in files:
        filename = file[0]
        filetime = file[1]["modify"]
        type = file[1]["type"]
        size = file[1]["size"]
        if type == "dir":
            filename += "/"
        print(size, datetime.strptime(filetime, "%Y%m%d%H%M%S"), filename , sep="\t")

def fetch_url(url, cache_dir, cache_time_limit, use_cache):
    cache_file = get_cache_file(url, cache_dir)
    if use_cache and os.path.exists(cache_file) and is_recent_file(cache_file, cache_time_limit):
        print(f"Using cached result from {cache_file}")
        return retrieve_cache(cache_file)
    else:
        response = requests.get(url)
        if response.status_code == 200:
            # save_cache(cache_file, response.text)
            return response.text
        else:
            response.raise_for_status()

def is_recent_file(file_path, cache_time_limit):
    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
    if datetime.now() - file_time <= timedelta(seconds=cache_time_limit):
        return True
    return False

def get_cache_file(url, cache_dir):
    sha256 = hashlib.sha256(url.encode('utf-8')).hexdigest()
    sub_dir = sha256[:3]
    cache_sub_dir = os.path.join(cache_dir, sub_dir)
    os.makedirs(cache_sub_dir, exist_ok=True)
    return os.path.join(cache_sub_dir, sha256)

def retrieve_cache(cache_file):
    with open(cache_file, 'r') as f:
        return f.read()

def save_cache(cache_file, content):
    with open(cache_file, 'w') as f:
        f.write(content)

if __name__ == "__main__":
    main()
